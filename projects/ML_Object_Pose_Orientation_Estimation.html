<html lang="en">
<title>Data Creation for Object Pose Orientation Estimation</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://www.w3schools.com/lib/w3-theme-black.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
    h1, h2, h3, h4, h5, h6 {
        font-family: "Roboto", sans-serif;
        color: teal;
    }
    html, body {
        font-family: "Roboto", sans-serif;
    }

    .w3-sidebar {
        z-index: 3;
        width: 250px;
        top: 43px;
        bottom: 0;
        height: inherit;
    }

    .introb {
        margin-top: 64px;
        margin-left: 75px;
        margin-right: 75px;
        margin-bottom: 12px;
    }
</style>
<body>

    <!-- Navbar -->
    <h1 id="data-creation-for-object-pose-orientation-estimation">Data Creation for Object Pose Orientation Estimation</h1>
    <p>Current pose estimation methods rely on multi key point identification. This is where the program looks for specified points in a 2D picture. The relative distance of those points can be used to calculate the location and orientation of objects. This approach is typically very sensitive to the environmental lighting conditions. Auto Encoders which can be used for object classification and recognition can be trained to be robust to these conditions. The current work that uses auto encoders for object pose estimation does this by using the latent code to implement a nearest-neighbors approach that seeks to match the observed image with a code book of saved images. Based on the relative distance the code book can be used to calculate averages to determine the approximate orientation. While this approach is promising it does not really include any understanding of orientation.</p>

    <p>The question then becomes, how does one train a neural network to have an inherent understanding of orientation, and what does an inherent understanding really mean? From a human perspective we simply approximate this to mean that we can successfully predict future orientations based on past ones. We begin by using some existing work (Word2Vec) by Google with word embeddings. Google's Word2Vec algorithm gives a vector representation to all words. This is used for translation among other things. Training is done by feeding a large amount of text to the Word2Vec algorithm.</p>

    <p>Results so far include we successfully finished the methodology for generating training data in the Word2Vec approach. This required determining a continuous space for orientations that represent all possible orientations. Due to the discontinuous nature of Euler angle representation and the continuous but dual representation of axis angle orientations, quaternions became the representation of choice. Quaternions represent a single continuous space, though that space is a 4-dimensional hypersphere. This required a methodology for traveling and sampling along the 4D hypersphere without violating hyper-spherical geometry constraints. The current approach is capable of generating successive orientation that are a result of traveling along the 4D hypersphere at various trajectories. This data can then be fed into Word2Vec where it can be used for orientating embedding.</p>

    <p style="color:teal;">Contact: William Harrison, (301) 975-4791</p>
</body>
