<h1 id="review-of-nist-process-to-apply-ai-and-ml-to-industrial-robots">Review of NIST Process to Apply AI and ML to Industrial Robots</h1>
<hr />
<h1 id="table-of-contents">Table of Contents</h1>
<p>1 <a href="#Abstract">Abstract</a></p>
<p>2 <a href="#Introduction">Introduction</a></p>
<p>2.1 <a href="#Criteria_for_Robotic_AI/ML_Applications">Criteria for Robotic AI/ML Applications</a></p>
<p>2.2 <a href="#Embodied_AI_and_Data_Generation_for_Manufacturing_Robotics">Embodied AI and Data Generation for Manufacturing Robotics</a></p>
<p>2.3 <a href="#Overview_of_Robot_Machine_Learning_Process_at_NIST">Overview of Robot Machine Learning Process at NIST</a></p>
<p>3 <a href="#Background_AI">Background AI</a></p>
<p>3.1 <a href="#Machine_learning">Machine learning</a></p>
<p>4 <a href="#Robotic_AI_Applications">Robotic AI Applications</a></p>
<p>4.1 <a href="#Reinforcement_Learning">Reinforcement Learning</a></p>
<p>4.2 <a href="#Grasping">Grasping</a></p>
<p>4.3 <a href="#Robot_Vision_and_Sensory_Processing">Robot Vision and Sensory Processing</a></p>
<p>4.4 <a href="#Highly_Redundant_Robot_Kinematic">Highly Redundant Robot Kinematic</a></p>
<p>5 <a href="#Performance_Testing_and_Evaluation">Performance Testing and Evaluation</a></p>
<p>5.1 <a href="#Criteria_for_Performance_Metrics">Criteria for Performance Metrics</a></p>
<p>5.2 <a href="#Sensor_readings,_Uncertainties,_and_Errors">Sensor readings, Uncertainties, and Errors</a></p>
<p>5.3 <a href="#Metrics">Metrics</a></p>
<p>5.4 <a href="#Software_metrics">Software metrics</a></p>
<p>5.5 <a href="#System_Trust">System Trust</a></p>
<p>6 <a href="#Performance_Testing">Performance Testing</a></p>
<p>6.1 <a href="#Functional_testing">Functional testing</a></p>
<p>6.2 <a href="#Cross-validation">Cross-validation</a></p>
<p>6.3 <a href="#Integration_testing">Integration testing</a></p>
<p>6.4 <a href="#Mitigating_AI/ML_Complete_Autonomy">Mitigating AI/ML Complete Autonomy</a></p>
<p>7 <a href="#Open_Data_Publishing">Open Data Publishing</a></p>
<p>7.1 <a href="#Open_Data_Publishing_Resources">Open Data Publishing Resources</a></p>
<p>7.2 <a href="#DOI">DOI</a></p>
<p>7.3 <a href="#ORCID">ORCID</a></p>
<p>7.4 <a href="#Code_Ocean">Code Ocean</a></p>
<p>7.5 <a href="#PapersWithCode">PapersWithCode</a></p>
<p>7.6 <a href="#FAIR">FAIR</a></p>
<p>7.7 <a href="#NIST">NIST</a></p>
<p>7.8 <a href="#ERB">ERB</a></p>
<p>7.9 <a href="#Computer_Software">Computer Software</a></p>
<p>7.10 <a href="#MIDAS">MIDAS</a></p>
<p>7.11 <a href="#Data.gov">Data.gov</a></p>
<p>7.12 <a href="#Metadata">Metadata</a></p>
<p>8 <a href="#DISCLAIMER">DISCLAIMER</a></p>
<p>9 <a href="#Glossary">Glossary</a>**</p>
<h1 id="abstract"><a name="Abstract"></a>Abstract</h1>
<p>Approaches categorized as &quot;artificial intelligence&quot; (AI) are enabling significant advances in robotics. Recently, data-centric machine learning has become a prominent tool in a number of disciplines relevant to robotics. AI applied to robotics &quot;can create smarter, faster, cheaper, and more environmentally-friendly production processes that can increase worker productivity, improve product quality, lower costs, and improve worker health and safety. Machine learning algorithms can improve the scheduling of manufacturing processes and reduce inventory requirements.&quot; AI's rapid rate of adoption has led to many successes, as well as the need for a measurement science infrastructure to help generate data and qualify it.</p>
<p>For industry to use AI, they must trust what comes out of the AI system. The key idea is to develop data sets and trained AI system, validated through performance evaluation techniques, to allow them to be applied to manufacturing robotic systems. This will allow manufacturers to gain more value from their robots by allowing the robot to &quot;learn&quot; new tasks, and how to better perform existing tasks, without the need for human intervention. NIST is uniquely qualified to address this because of our experience in robot performance characterization, information modeling standards, and robot programming.</p>
<p><strong>Key words:</strong> robotics, machine learning, artificial intelligence, neural networks, manufacturing</p>
<h1 id="introduction"><a name="Introduction"></a>Introduction</h1>
<p>Approaches categorized as &quot;artificial intelligence&quot; (AI) are enabling significant advances in robotics. These include symbolic logic, Bayesian statistics, and numerous other algorithmic approaches. Recently, data-centric machine learning has become a prominent tool in a number of disciplines relevant to robotics. AI applied to robotics &quot;can create smarter, faster, cheaper, and more environmentally-friendly production processes that can increase worker productivity, improve product quality, lower costs, and improve worker health and safety. Machine learning algorithms can improve the scheduling of manufacturing processes and reduce inventory requirements.&quot; AI's rapid rate of adoption has led to many successes, as well as the need for a measurement science infrastructure to help generate data and qualify it. For industry to use AI, they must trust what comes out of the AI system. The key idea is to develop data sets and trained AI system, validated through performance evaluation techniques, to allow them to be applied to manufacturing robotic systems. This will allow manufacturers to gain more value from their robots by allowing the robot to &quot;'learn'' new tasks, and how to better perform existing tasks, without the need for human intervention. NIST is uniquely qualified to address this because of our experience in robot performance characterization, information modeling standards, and robot programming.</p>
<h2 id="criteria-for-robotic-aiml-applications"><a name="Criteria_for_Robotic_AI/ML_Applications"></a>Criteria for Robotic AI/ML Applications</h2>
<p>Using AI/ML in physical systems is a daunting task. Errant behavior cannot be tolerated as efficacy as well as safety of expensive equipment are of important. Robots can be dangerous to themselves and others, especially those with deficient or poorly tested AI/ML algorithms. Thus, a key scientific and practical goal is to characterize the confidence of a given robotic AI/ML algorithm. This leads to performance questions. For ML, is the volume of training data sufficient to learn and predict the expected behavior? How robust is the AI/ML to errors in the AI/ML model and the training data?</p>
<p>Then issues related to developing an AI/ML application need to be addressed that is suitable to a robotics application. Is it possible to successfully train, test and evaluate the efficacy of an AI/ML algorithm or is the robotics learning problem fundamentally intractable?</p>
<p>Such theoretical characterizations of machine-learning algorithms and problems typically make use of the familiar frameworks of statistical decision theory and computational complexity theory [<a href="#Reference_1">1</a>]. Attempts to characterize general machine-learning algorithms have used statistical and computational theory in which the goal is to simultaneously characterize the sample complexity (how much data are required to learn accurately) and the computational complexity (how much computation is required) and to specify how these depend on features of the learning algorithm such as the representation it uses for what it learns [<a href="#Reference_2">2</a>][<a href="#Reference_3">3</a>] .</p>
<p>It is typically out of the realm of robot application engineer to ponder the data and computational complexity, but rather the intention is to leverage existing ML technologies and develop robot applications that are tractable and robust while providing predictable and safe behavior. For a robot AI/ML application there need to be input, output, and timing guards and/or filters to prevent erratic or unsafe behavior. Kober et. al. point out it is often unrealistic to assume that the true state of a robot is completely observable and noise free, and therefore must often use a filter to estimate the true state[<a href="#Reference_4">4</a>]. Further for reinforcement learning training of an AI/ML system using a real physical system can be difficult and hard to reproduce.</p>
<h2 id="embodied-ai-and-data-generation-for-manufacturing-robotics"><a name="Embodied_AI_and_Data_Generation_for_Manufacturing_Robotics"></a>Embodied AI and Data Generation for Manufacturing Robotics</h2>
<p>The Embodied AI and Data Generation for Manufacturing Robotics is a NIST project to explore data generating mechanisms (physical and virtual) across a variety of relevant experimental factors relevant to industrial robotic applications. Generated data will be organized and formatted in a way that will facilitate the training of AI applications in robotic software agility, perception, grasping, teaming, and mobility, and will include metadata that will define the data's scope and applicability. The datasets and the associated approaches will be validated through its implementation on a robot, showing how the robot can directly solve a manufacturing task or improve existing performance.</p>
<p>The overreaching goal is the public sharing and collaboration of all elements of the NIST AI/ML robotics work. The sharing includes text, data sets, metadata, as well as algorithms. The generated data will be made available to researchers and industry practitioners to train and test AI systems for industrial robotic applications. The specifics regarding learning methods, settings, and code bases will be disseminated through publications, online data sets and open source code repositories.</p>
<h2 id="overview-of-robot-machine-learning-process-at-nist"><a name="Overview_of_Robot_Machine_Learning_Process_at_NIST"></a>Overview of Robot Machine Learning Process at NIST</h2>
<p>The public sharing of the NIST AI/ML robotics work will at the same time conform to both NIST and robotics research collaboration considerations. Before any results can be disseminated at NIST, there is a formal reviewing process that is required before one can disseminate any research.  REF _Ref14162676  shows the high-level flow chart required to produce and disseminate the results of a Machine Learning robotics application at NIST.</p>
<div class="figure">
<img src="./images/AIPaper_image1.gif" alt="Figure1" /><p class="caption">Figure1</p>
</div>
<p align="center">


<p><em>Figure  SEQ Figure * ARABIC  Flow Chart of Robot Machine Learning Development and Dissemination at NIST</em></p>
</p>


<p>The first step in the process is to identify a robot application worthy of AI/ML. In this regard, a candidate AI/ML task would be one that solves or simplifies a given robot function. For example, assume we have an especially complicated robot that is difficult to program or difficult to maintain, should a potential AI/ML solution exist, then this would be a worthy solution candidate.</p>
<p>Next, assuming an AI/ML robot application has been determined, a class of AI/ML neural net and the ML programming environment and toolkit needs to be selected and matched to the functional requirements. There are many capabilities as well as programming languages to be found in existing AI/ML tools. Some of the functionality available in various AI/ML tools include: classification, regression, clustering, data preprocessing, dimensionality reduction, data flow, massive data sets, etc. In addition, the ability to use parallel programming technology (e.g., Graphics processing unit (GPU)) may be imperative for improving the neural net training speed or the AI/ML application may be computationally intractable. To that note, NIST has a high performance computing platform called Enki which consists of a 13-node IBM Power9 system to assist in scientific computations driven by massive data sets, such as for AI/ML [<a href="#Reference_5">5</a>].</p>
<p>Data set generation is an important element in building a successful AI/ML robot application. The data set sample complexity, size, and statistical distribution are among many of the concerns in training a neural network. There are abundant datasets to be found on the Internet [<a href="#Reference_6">6</a>] [<a href="#Reference_7">7</a>] [<a href="#Reference_8">8</a>] [<a href="#Reference_9">9</a>]. However, a stated goal of the NIST AI/ML project is to generate useful robotic datasets. There are two main types of datasets, either real or synthetic. A real dataset is generated from observing and recording actual real world events or experimental data. A synthetic dataset is generated programmatically or by a simulation. This step may be an iterative process, as although the application may seem suitable, generating the appropriate data may prove unfeasible.</p>
<p>Once an application and a neural net programming framework has been selected, training and evaluation of the AI/ML application is performed. We assume a neural network is the machine learning algorithm used. Neural networks are one of the most common machine learning algorithms and are particularly good when applied to problems where there is a large amount of input data. Through an input layer, one or more hidden layers, and an output layer, a neural network works by connecting up a series of neurons with weights assigned to each connection. As each connection is activated, a calculation is performed on the connection before passing through an activation function at each level of the hidden layers. Neural networks can be used without knowing precisely how the training operates, however, currently setting up the neural network requires an understanding of programming ML algorithms to consume training and evaluation data set.</p>
<p>The next step is to verify the correctness of the AI/ML component inside a bigger robotic system. At this point, performance testing turns from a virtual world into a physical world with the robot to evaluate the efficacy and correctness of the AI/ML neural network. The ability to comprehensively evaluate the quantitative and qualitative performance of an AI/ML system is critical to accurately predicting how it will perform in various situations. The performance evaluations is often as much of a challenge as is the design of the AI/ML applications themselves. Visualization is important for verifying the basic functionality correctness of the AI/ML. Clearly, if a simulated visualization has truly negative consequences, e.g., software crashes or timing issues such as jitter or produces errant behavior, then the AI/ML application is flawed. A systematic strategy to examine the AI/ML fidelity will be presented later. As in all software, there can exist bugs that are not readily apparent. Further, large output divergence resulting from close inputs can be particularly onerous as an expected &quot;routine&quot; answer can be catastrophic so safeguards must be in place.</p>
<p>In order to publish the AI/ML data, code and papers, a NIST approval process is required and it entails a multistep operation. Minimally, NIST Gaithersburg publications must go through a Washington Editorial Review Board (WERB) process to insure compliance to NIST publishing standards. Associated with the publication and in our case potentially more important, data now needs to go through a NIST MIDAS review process to ensure compliance with NIST data standards. Every published NIST data and article now has a DOI established with each publication, either data or article. In addition, academic societies, such as IEEE, have been working to establish reproducible robotics research through code publishing (using Code Ocean) as well as publication guidelines through Good Experimental Methodology (GEM).</p>
<h1 id="background-ai"><a name="Background_AI"></a>Background AI</h1>
<p>Artificial intelligence is the study of ideas that enable computers to be intelligent [<a href="#Reference_10">10</a>]. Since the formal definition of intelligence is difficult but the concept is intuitive, emphasizing the role of intelligent computing suffices to describe the overreaching goal of AI. AI itself has many applications.  REF _Ref14162516  sketches some of the major capabilities under the umbrella of artificial intelligence.</p>
<div class="figure">
<img src="./images/AIPaper_image2.gif" alt="Figure2" /><p class="caption">Figure2</p>
</div>
<p align="center">


<p><em>Figure  SEQ Figure * ARABIC General Capabilities of AI</em></p>
</p>


<p>AI is used for reasoning about problems, such as manipulating blocks in a blocks world that at a more practical level corresponds to robot grasping and part assembly. AI is also used for exploring alternative routes to a problem solution using searching and backtracking. Searching techniques can be exhaustive, evolutionary, smart, and/or pruned. Such functionality is useful for planning and in games. Problem solving is another key AI area where a generate-and-test or a rule-based-systems can be applied to computer architecture layout, health diagnoses, and game playing Such system are known as &quot;expert systems&quot; that uses rules to make deductions or choices. Knowledge representation is another domain under the purview of AI often implemented as semantic nets to model natural language as well as human understanding.</p>
<p>Learning is a branch of artificial intelligence based on the idea that systems exhibit adaptive behavior and can learn from data, identify patterns and make decisions. Machine learning relies on a method of data analysis that facilitates model building.</p>
<p>Artificial intelligence as applied to robotics includes sensing, navigation, path planning, and control as these areas require programming and by definition are artificial, while at the same time are intrinsically linked to a smarter more intelligent robot.</p>
<p>Robots typically include perception so that robot control for reaching and moving objects in a complex environment with obstacles and vision occlusions is a leading AI application [<a href="#Reference_11">11</a>].</p>
<h2 id="machine-learning"><a name="Machine_learning"></a>Machine learning</h2>
<p>Webster dictionary defines learning as the ability to gain knowledge or understanding of or skill in by study, instruction, or experience; or as the modification of a behavioral tendency by experience (such as exposure to conditioning) [<a href="#Reference_12">12</a>]. Of interest in this definition, is the ability to gain an understanding through experience. For our purposes, we refine the term experience to be data training. This leads to the following machine learning definition.</p>
<p>Artificial Intelligence is a more general concept of &quot;smarter&quot; machines while machine learning is an AI application that &quot;smart&quot; machines continually adapt to data themselves. The most frequently cited definition of learning comes from Simon [<a href="#Reference_13">13</a>]: &quot;Learning denotes changes in the system that is adaptive in the sense that they enable the system to do the same task or tasks drawn from the same population more effectively the next time&quot;. But there can be a more static viewpoint of machine learning. One can use machine learning and big data techniques to train neural net to be intelligent and make intelligent predictions, without continuously adapting to the environment and improving. However, this does assume that the trained model is sufficiently intelligent for the application.</p>
<p>Variations on machine learning terminology and implementation exist. Some of the terminology and implementations overlap. The following sketches some of the most salient learning concepts.</p>
<p><strong>Deep learning</strong> Deep learning is the science of training large artificial neural networks [<a href="#Reference_14">14</a>]. Deep neural networks (DNNs) form compact representations from raw, high-dimensional, multimodal sensor data commonly found in robotic systems[<a href="#Reference_15">15</a>].</p>
<p><strong>Supervised Learning</strong> Type of learning in which the data outcome is known, and this data outcome is explicitly used during training that is the model is trained under the supervision of a teacher. For example, if we want to build a classification model for handwritten digits, the input will be the set of images (training data) and the target variable will be the labels assigned to these images, that is their digit value outcomes from 0-9 associated with each image.</p>
<p><strong>Unsupervised Learning</strong> Type of learning in which no supervisor is involved. The goal is to directly infer the properties of a dataset without the help of a supervisor providing correct answers for each input.</p>
<p><strong>Imitation learning</strong> Type of learning in which the goal is to train by replicating the observed behavior and is typically achieved using supervised learning techniques.</p>
<p><strong>Reinforcement learning</strong> Type of learning algorithm in which the machine takes decisions on what actions to take, given a certain situation/environment, so as to maximize a reward. In reinforcement learning, learner is not told which actions to take, as in most forms of machine learning, but instead, must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also subsequent future rewards.</p>
<h1 id="robotic-ai-applications"><a name="Robotic_AI_Applications"></a>Robotic AI Applications</h1>
<p>The goal of AI is to develop worthwhile robotic applications that traditional programming approaches have had difficulty. Typically, a robot software system is divided into planning, sensing and control. Robot applications typically mimic humans, such as pick and place requiring dexterous grasping, planning, visual sensing and tracking, obstacle avoidance. This section will review some of the current robotic AI/ML research.</p>
<h2 id="reinforcement-learning"><a name="Reinforcement_Learning"></a>Reinforcement Learning</h2>
<p>Reinforcement Learning (RL) is a machine learning framework for optimizing the behaviour of an agent interacting with an unknown environment <span class="citation">[@sutton1998introduction]</span>. Reinforcement Learning enables a robot to autonomously discover an optimal behavior through trial-and-error interactions with its environment[<a href="#Reference_16">16</a>] <span class="citation">[@kormushev2013reinforcement]</span>. Instead of explicitly detailing the solution to a problem, in reinforcement learning the designer of a control task provides feedback in terms of an objective function that measures the one-step performance of the robot.</p>
<p>Reinforcement learning enables a robot to autonomously discover an optimal behavior through trial-and-error interactions with its environment [<a href="#Reference_16">16</a>]. Instead of explicitly detailing the solution to a problem, in reinforcement learning the designer of a control task provides feedback in terms of an objective function that measures the one-step performance of the robot.</p>
<p>Larouche and Féraud [<a href="#Reference_17">17</a>] reinforcement] defines reinforcement Learning (RL) to be considered learning through trial and error to control an agent behavior in a stochastic environment: at each time step <img src="https://latex.codecogs.com/svg.latex?t&space;\in&space;N" title="t&space;\in&space;N" /> , the agent performs an action <img src="https://latex.codecogs.com/svg.latex?a(t)&space;\in&space;A" title="a(t)&space;\in&space;A" /> , and then perceives from its environment a signal <img src="https://latex.codecogs.com/svg.latex?o(t)&space;\in&space;\omega" title="o(t)&space;\in&space;\omega" /> called observation, and receives a reward <img src="https://latex.codecogs.com/svg.latex?t(t)&space;\in&space;R" title="t(t)&space;\in&space;R" /> $, bounded between <img src="https://latex.codecogs.com/svg.latex?R_{min}" title="R_{min}" /> and <img src="https://latex.codecogs.com/svg.latex?R_{max}" title="R_{max}" />. Laroche and Féraud then proposes to share their trajectories expressed in a universal format. A high level definition of the RL algorithms allows to share trajectories between algorithms: a trajectory as a sequence of observations, actions, and rewards can be interpreted by any algorithm in its own decision process and state representation.</p>
<p>Kober [<a href="#Reference_4">4</a>] uses training a robot to play table tennis to explain RL concepts. Robot observations of ball position and velocity as well as the internal joint dynamics constitute the state <em>s</em> of the system. The actions <em>a</em> available to the robot could be torque motor commands. A function <img src="https://latex.codecogs.com/svg.latex?\pi" title="\pi" /> generates the actions based on the state and would be called a policy. This leads to the definition of a reinforcement problem is to find a policy that optimizes the long-term sum of reward <img src="https://latex.codecogs.com/svg.latex?R(s,a)" title="R(s,a)" /> .</p>
<h2 id="grasping"><a name="Grasping"></a>Grasping</h2>
<p>The broader goal of robot grasping is to develop highly reliable robot grasping across a wide variety of rigid objects such as tools, household items, packaged goods, and industrial parts.</p>
<p>The Dexterity Network (Dex-Net) is a research project at the University of California Berkeley Automation Lab whose goal is to generate datasets of synthetic point clouds, robot parallel-jaw grasps and metrics of grasp robustness based on physics for thousands of 3D object models to train machine learning-based methods to plan robot grasps[<a href="#Reference_18">18</a>] <span class="citation">[@mahler2017learning; @dexnet]</span>. Several generation of Dex-Net have evolved adding various grasping skills. Of distinction, Dex-Net 2.0 is designed to generated training datasets to learn grasping models that predict the probability of success of candidate parallel-jaw grasps on objects from point clouds. The goal was for a robot to quickly plan grasps for a wide variety of objects.</p>
<h2 id="robot-vision-and-sensory-processing"><a name="Robot_Vision_and_Sensory_Processing"></a>Robot Vision and Sensory Processing</h2>
<p>Machine learning with deep convolutional neural networks has become a primary method when it comes to image classification problems. Such sensory AI technology need not be limited to image sensor processing, but could be applied to other sensor applications such as tactile sensing or joint encoders[<a href="#Reference_15">15</a>].</p>
<h2 id="highly-redundant-robot-kinematic"><a name="Highly_Redundant_Robot_Kinematic"></a>Highly Redundant Robot Kinematic</h2>
<p>Highly redundant robots are prime candidates for machine learning to solve the inverse kinematics. Especially for redundant robots, inverse kinematics algorithms need to address how to determine a particular solution in face of multiple solutions.</p>
<p>In [<a href="#Reference_19">19</a>], a statistical learning algorithm, &quot;Locally Weighted Projection Regression&quot;, is shown to exhibit efficient learning of inverse kinematic mappings in an incremental fashion even when input spaces are high dimensional.</p>
<h1 id="performance-testing-and-evaluation"><a name="Performance_Testing_and_Evaluation"></a>Performance Testing and Evaluation</h1>
<p>One of the problems with the application of a Neural Net AI technology as opposed to a sequential programming approach is the trustworthy factor associated with how the result was derived. With sequential programming you can go step by step to establish a chain of accountability. With neural nets, you rely that all answers are reasonable, and it is difficult to do 100% testing coverage of a neural net.</p>
<h2 id="criteria-for-performance-metrics"><a name="Criteria_for_Performance_Metrics"></a>Criteria for Performance Metrics</h2>
<p>The goal of assessing AI/ML performance is to achieve a situation in which users and providers of AI/ML have an accurate assessment of the performance and reliability of the AI/ML that they use/provide. To achieve this, performance and reliability metrics for AI/ML algorithms and implementations must be developed. The general requirements for these metrics include [<a href="#Reference_20">20</a>]:</p>
<ul>
<li><p>A metric should have the property that it is repeatable: if the methodology is used multiple times under identical conditions, the same sensor readings should result in the same sensor readings. Of note, this may not be possible or desirable of continuous machine learning that adapts to new and different experiences.</p></li>
<li><p>The metrics must exhibit no bias for implementations of identical technology,</p></li>
<li><p>The metrics must be useful to users and providers in understanding the performance they experience or provide,</p></li>
</ul>
<h2 id="sensor-readings-uncertainties-and-errors"><a name="Sensor_readings,_Uncertainties,_and_Errors"></a>Sensor readings, Uncertainties, and Errors</h2>
<p>Ideally a sensor reading is both accurate and precise, with sensor readings all close to and tightly clustered around the true value. The accuracy and precision of a sensor reading process is usually established by repeatedly measuring some traceable reference standard. Traceability to a widely accepted standard permits comparisons of sensor readings among users. Reliability refers to the consistency of accurate results over consecutive sensor readings over time [<a href="#Reference_21">21</a>]. As background, there are numerous terms used in assessing the performance of systems and sensor reading results. Accuracy, precision, trueness, bias, error, variability, tolerance, traceability, reliability, and uncertainty are each integral to understanding performance metrics.</p>
<p><strong>Performance</strong> is characterized by a sensor reading, which not matter how exact is usually taken as the estimate [<a href="#Reference_22">22</a>]. This leads to the following definitions:</p>
<p><strong>Trueness</strong> is the closeness of a sensor reading result to the actual (true) value.</p>
<p><strong>Error</strong> is the difference between a measured value of quantity and its true value.</p>
<p><strong>Variability</strong> is an inherent part of sensor reading systems and sensor readings - no two sensor readings can be expected to be the same.</p>
<p><strong>Error</strong> There are two types of sensor reading errors: systematic error and random error.</p>
<p>systematic error is the error associated with the fact that a sensor reading contains a consistent error.</p>
<p>Random error is caused by inherently unpredictable fluctuations with the sensor reading and when a sensor reading is repeated it will generally provide a measured value that is different from the previous value.</p>
<p><strong>Uncertainty</strong> Sensor Reading uncertainty characterizes the dispersion of sensor reading values to the true value.</p>
<p><strong>Accuracy</strong> of a sensor reading is the degree of closeness of sensor readings of a quantity to the true value.</p>
<p><strong>Precision</strong> is the closeness of agreement among a set of sensor reading results. Precision can also be defined with reference to a sensor reading number of decimal places. Most obvious, representing a sensor reading with a millionth decimal place versus a tenth decimal place provides a more exacting result, but there is no guarantee that extra decimal places add accuracy.</p>
<p><strong>Repeatability</strong> is the closeness of the agreement between the results of successive sensor readings of the same measurand carried out under the same conditions of sensor reading [<a href="#Reference_23">23</a>]<span class="citation">[@jcgm2008evaluation]</span>.</p>
<p>Ideally a sensor reading is both accurate and precise, with sensor readings all close to and tightly clustered around the true value. The accuracy and precision of a sensor reading process is usually established by repeatedly measuring some traceable reference standard. Traceability to a widely accepted standard permits comparisons of sensor readings among users. Reliability refers to the consistency of accurate results over consecutive sensor readings over time [<a href="#Reference_21">21</a>].</p>
<h2 id="metrics"><a name="Metrics"></a>Metrics</h2>
<p>In the application of AI/ML to robotics, there are several issues related to the performance and reliability. The metrics of interest are performance and correctness based, and not for instance, programmer productivity metrics, software management metrics, algorithmic costing metrics, or software design metrics.</p>
<p>Since the application domain is robotics, in general, real-time performance is required. This means that a response needs a hard time bound or a response can be catastrophic (e.g., collision), or flawed (shaky behavior due to intermittent and non-real-time response times) or deadly (e.g., robot in motion but not under sufficiently robust timed control that could hurt someone). Given proper safeguards.poor response times can be tolerated (e.g., have a graceful degradation if response exceeds threshold) unless AI/ML response time continually overruns any time constraint.</p>
<p>A bigger concern to robotics application is the correctness of an AI/ML response. AI/ML is a proverbial black box, with a predicted output from a given input, but there is no guarantee of of a nearness of predicted output response to a close inputs. Further if the internal AI/ML algorithm in engaged in continually adaptive learning to new and different experiences, sampled input testing of the AI/ML cannot guarantee correctness as the input/output, stimulus/response, can change over time as the AI/ML algorithm learns. Thus, in traditional programming, there may be bugs in the code but there is a certainty to a response output to a given input. Suc that a well-tested traditional software program should operate as expected, omitting underlying software changes to operating system, libraries or robot hardware and devices.</p>
<h2 id="software-metrics"><a name="Software_metrics"></a>Software metrics</h2>
<p>There are nuances to AI/ML software correctness.</p>
<p>An AI/ML predicted outcome may correctly identify an object in an image, but the position and orientation of the object may have varying degrees of errors. If the goal is to successfully identify the object, then the AI/ML succeeded. However, if a robot were to grasp an object given and incorrect position and orientation, then the AI/ML failed.</p>
<p>If we assume AI/ML is continually learning and thus adapting, this could lead to different results to the same inputs over time. However, if these results are consistent (or better) with previous trials, then the AI/ML should be deemed correct.</p>
<p>Given this dichotomy of behavior, this leads to metrics of software correctness success with side effects that can impact correctness.</p>
<p><strong>Reliability</strong> metric that is the probability that the software will perform its intended function for a specified interval under stated conditions [<a href="#Reference_24">24</a>]. Failures occur due to weaknesses in the design, bugs in the code, bad training data, flaws in the algorithm, defects from the manufacturing processes, improper application, etc. This assumes that sensor reading of the operation returns 100% success rate, which</p>
<p><strong>Consistent and unambiguous</strong> A software correctness concern with software quality is the potential for identical outcomes from different inputs. For example, redundant robot manipulators can have several joint solutions to attain an identical robot pose. This violates a 1:1 input to output mapping. (Is this bad?) Thus, it is imperative that further input qualifications (such as configuration flags) be assigned to the inputs to differentiate the trained machine learning model.</p>
<p><strong>Quality Assumptions</strong> affecting the software quality are not always valid Some software quality assumptions will be listed. Software fault are independent. Inputs for testing software performance should be randomly selected from an input space. We assume the test space is representative of the operational input space. We assume there is a closeness or continuity between near inputs and the resulting outputs. We assume software failure is observable. We further assume faults are corrected without introducing new faults.</p>
<p>In order to estimate the AI/ML program reliability and quality we can use test cases sampled from input.</p>
<p>The test cases could be software test coverage, which measures the degree to which the source code of a program is successfully executed when a particular test suite runs. A program with high test coverage has had more of its source code executed during testing, which suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage [<a href="#Reference_25">25</a>].</p>
<p>Test cases based on a design of experiments where the variance of inputs plays a vital role in establishing correctness through analysis of variance. Design of experiments may be required, since all combinations of inputs may not be possible to test such that some form of partitioning like inputs into equivalence classes is required before running tests on combinations of these grouped inputs. It does assume the outcome of test case for other points close to test points is similar. Taguchi introduced levels of noise into the input test cases to improve experiment analysis fidelity [<a href="#Reference_26">26</a>].</p>
<h2 id="system-trust"><a name="System_Trust"></a>System Trust</h2>
<p>People who use AI/ML systems want to be able to trust such systems - that is, to know that these systems will perform correctly and to understand the reasoning behind the system's actions. A synthesis of research reveals that AI/ML system trust fall along a spectrum in terms of assurance [<a href="#Reference_27">27</a>].</p>
<p>AI/ML systems are designed to be trained from large datasets and in some cases are expected to adapt to new never before seen data. However, the correctness of AI/ML systems is in question if such data does not conform to assumptions made at design time. To handle this issue before AI/ML processing, traditionally programmed data validation in accordance with the design criteria may be required.</p>
<h1 id="performance-testing"><a name="Performance_Testing"></a>Performance Testing</h1>
<p>The purpose of this section is to outline the steps necessary to verify and validate that the AI/ML software satisfies all functional Requirements.  REF _Ref14162407  shows the testing process of an AI/ML application module. Issues associated with verifying each step are included as validation inputs. In the sections that follow a more thorough discussion on performance testing will be discussed.</p>
<div class="figure">
<img src="./images/AIPaper_image3.gif" alt="Figure3" /><p class="caption">Figure3</p>
</div>
<p align="center">


<p><em>Figure  SEQ Figure * ARABIC  Performance Testing Elements</em></p>
</p>


<p>Inherent to the AI/ML module is a dependence on machine learning frameworks. There are many AI frameworks, in many different programming flavors. Both commercial and open source have a spate of active users of these frameworks. As such, we will assume any issues with underlying AI software would be incumbent on the AI/ML model under test and not on the framework used to generate the machine learning models.</p>
<h2 id="functional-testing"><a name="Functional_testing"></a>Functional testing</h2>
<p>Functional testing is performed when the AI/ML application is complete. Functional testing is the process of running Test Cases to verify the application meets the functional objectives. Typically, AI/ML functional testing uses a part of the dataset as a test set to verify functional correctness. In addition, the interface to the AI/ML module may require various test sequences to verify operation.</p>
<p>Goodness of fit often refers to measures used to estimate how well the approximation of the function matches the target function. For functional testing, there are two goodness of fit problems: overfitting and underfitting.</p>
<p>Overfitting refers to an AI/ML model that learns the training data too well. This effect means that noise or random fluctuations in the training data is picked up and learned as concepts by the AI/ML model. The problem is that these concepts do not apply to new data and negatively impact the model's ability to generalize.</p>
<p>Underfitting refers to a model that can neither model the training data nor generalize to new data. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. Underfitting is often easy to detect as output results will lead to poor performance.</p>
<h2 id="cross-validation"><a name="Cross-validation"></a>Cross-validation</h2>
<p>The neural network dataset terminology has the following concepts <span class="citation">[@batchsizeNN]</span>:</p>
<ul>
<li><p>one epoch = one forward pass and one backward pass of all the training examples</p></li>
<li><p>batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.</p></li>
<li><p>number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).</p></li>
</ul>
<p>For example, if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.</p>
<p>Cross validation is a technique to evaluate the efficacy of an AI/ML model's performance. It is common practice when performing a (supervised) machine learning experiment to split out part of the available data as a test dataset.</p>
<p>Cross-validation is an AI/ML methodology to validate a generated model against test data [<a href="#Reference_28">28</a>]. There are several ways to split the test data from the data set. There is a trade off between exhaustive testing and performance. There are several methods of splitting a dataset for cross-validation [<a href="#Reference_29">29</a>]:</p>
<ul>
<li><p>Holdout Method: Using a percentage of the dataset as the test data</p></li>
<li><p>K-Fold Method: Divides data into multiple (i.e., K) sections (i.e., Folds), then uses each as a test/train dataset.</p></li>
<li><p>Leave-P-Out Method: Using every combination of a number of points (P) as test data</p></li>
</ul>
<h2 id="integration-testing"><a name="Integration_testing"></a>Integration testing</h2>
<p>Integration testing is the process of verifying the interfaces between system components and the AI/ML module. Regression testing focuses on executing the robot functional tests and see if the AI/ML module reacts favorably. Defects could be poor timely response or functional errors due to issues such as process starvation, poor threading code. It is assumed that an improper response from the AI/ML module would have been detected in functional testing, but issues such as timing, threading, preemption, starvation could magnify borderline issues.</p>
<h2 id="mitigating-aiml-complete-autonomy"><a name="Mitigating_AI/ML_Complete_Autonomy"></a>Mitigating AI/ML Complete Autonomy</h2>
<p>We assume AI/ML are mandated to operate with a degree of autonomy. The question arises can filters be applied pre-and-post processing to the AI/ML black box to double check inputs and outputs, and guard against Clearly there are various operational environments which dictate the reaction to errant behavior. Graceful shutdown under controlled circumstances may be acceptable versus graceful transition to safe mode under highly dangerous situations may be required.</p>
<h1 id="open-data-publishing"><a name="Open_Data_Publishing"></a>Open Data Publishing</h1>
<p>A new and ongoing development in robotics and automation is the goal to achieve reproducible and measurable robotic research. The IEEE has even established a Technical Committee on Performance Evaluation and Bench marking of Robotics and Autonomous System that has elements for defining reproducible papers. The reproducibility of experimental results is a key characteristic of the scientific method. An ongoing concern within the robotics research community is the difficulty of reproducing results which slows down the industrial take-up of new solutions. Basic research is also hindered, since it is very difficult for a research group to build on the results of another one, leading to a very limited cross-exploitation of results between different groups, and a general prevalence of exploration over exploitation.</p>
<p>Overall, the replications of robotics and artificial intelligence (AI) experiments has been limited [<a href="#Reference_30">30</a>]. This fact hampers both research progress and results utilization[<a href="#Reference_31">31</a>][<a href="#Reference_32">32</a>] when the goal is to regard (intelligent) robotics as a science.</p>
<h2 id="open-data-publishing-resources"><a name="Open_Data_Publishing_Resources"></a>Open Data Publishing Resources</h2>
<p>Global open data publishing resources are fostering a new era of global sharing and collaboration of research. By leveraging connections between local infrastructures and global information resources, data sources that were previously either unavailable or prohibitively labor-intensive are now publicly available. To assist this cause, technologies described subsequently are part of the transition to an automated collaboration research environment.</p>
<p>Some of the more noteworthy resources to enable public sharing and collaboration of research are highlighted. Tool identifiers as ORCID or DOI are being embedded in research workflows either data or written word. A tool such as Code Ocean provides a publicly accessible data storage facility to share algorithms. FAIR provides a software tool to assess the openness of a collaborative resource.</p>
<h2 id="doi"><a name="DOI"></a>DOI</h2>
<p>In computing, a Digital Object Identifier or DOI is a persistent identifier used to identify objects uniquely, standardized by the International Organization for Standardization (ISO) [<a href="#Reference_33">33</a>]. DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.</p>
<h2 id="orcid"><a name="ORCID"></a>ORCID</h2>
<p>ORCID provides a persistent digital identifier that distinguishes the author from other researchers and, through integration in key research workflows such as manuscript and grant submission[<a href="#Reference_34">34</a>]. ORCID is a non-profit organization supported by a global community of organizational members, including research organizations, publishers, funders, professional associations, and other stakeholders in the research ecosystem. ORCID is part of the wider digital infrastructure needed for researchers to share information on a global scale. ORCID enables transparent and trustworthy connections between researchers, their contributions, and affiliations by providing an identifier for individuals to use with their name as they engage in research, scholarship, and innovation activities. ORCID is useful for determining a publication impact factor as a means to measure the influence of research journals and researchers, which are based on number of citations of a given publication.</p>
<p>The web site [<a href="#Reference_35">35</a>] explains the difference between DOI and ORCID. A researcher can be uniquely identified by her or his unique ORCID, while each publication can be uniquely identified by its DOI.</p>
<h2 id="code-ocean"><a name="Code_Ocean"></a>Code Ocean</h2>
<p>Today's research often includes software code, statistical analysis and algorithms that are not included in traditional publishing. But they are often essential to reproducing the research results and reusing them in a new product or research. This creates a major roadblock for researchers that lead to the development of Code Ocean. Code Ocean is a research collaboration platform that provides researchers and developers a way to create, share, and run code for private projects that are published in academic publishing venues [<a href="#Reference_36">36</a>]. Now, researchers can upload code and data in any open source programming language and link the working code in a computational environment with the associated article for free. A Digital Object Identifier (DOI) is associated with the algorithm and data, providing correct attribution and a connection to the published research. Thus, anyone can run an algorithm posted to Code Ocean, verify it, modify it, and test the modifications.</p>
<p>The Institute of Electrical and Electronics Engineers (IEEE) has established a link between research papers and research code with the ability to publish code. Any author that has had an IEEE journal article published can upload associated algorithms to Code Ocean by visiting  HYPERLINK &quot;https://codeocean.com/ieee/signup&quot; &quot;_blank&quot;  [<a href="#Reference_37">37</a>]. Once the algorithm is uploaded to Code Ocean, it will be automatically linked to the associated article in IEEE.</p>
<h2 id="paperswithcode"><a name="PapersWithCode"></a>PapersWithCode</h2>
<p>The mission of Papers With Code is to create a free and open Web-based resource for Machine Learning papers, code and evaluation tables [<a href="#Reference_38">38</a>]. Papers With Code has a broad coverage of Machine Learning research with more than 60,000 papers. In addition, Papers With Code has also manually annotated tasks and datasets in 1,600 ArXiv abstracts.</p>
<p>Papers With Code uses software automation to catalogue the machine learning papers and related code and data information [<a href="#Reference_38">38</a>], and is working on automating the extraction of evaluation metrics from papers. Papers With Code web links include 1) papers with abstracts, 2) links between papers and code, and 3) evaluation tables.</p>
<h2 id="fair"><a name="FAIR"></a>FAIR</h2>
<p>Findable, Accessible, Interoperable and Reusable (FAIR) is a self-assessment tool that is available through the NIST Library and can be found at [<a href="#Reference_39">39</a>]. FAIR was developed by the Australian Research Data Commons (ARDC) and it is a self-assessment tool that can assess the 'FAIRness' of a dataset and determine how to enhance its FAIRness (where applicable). It is a self-assessment tool that is available through the NIST Library. FAIR has been designed predominantly for data librarians and IT staff but could be used by software engineers developing FAIR Data tools and services, and researchers provided they have assistance from research support staff.</p>
<p>FAIR asks questions related to the principles underpinning Findable, Accessible, Interoperable and Reusable. Once you have answered all the questions in each section you will be given a 'green bar' indicator based on your answers in that section, and when all sections are completed, an overall 'FAIRness' indicator is provided.</p>
<h2 id="nist"><a name="NIST"></a>NIST</h2>
<p>NIST provides an information technology (IT) solution to enable the shared storage, exchange, and dissemination of NIST research data worldwide.  REF _Ref14162243 * MERGEFORMAT  shows the Open Access to Research (OAR) to handle data and paper publishing at NIST. Data publishing at NIST has evolved into formal procedure that includes preparing your data for publication, using MIDAS, and enhancing your data publication so that it can be more easily used and cited.</p>
<div class="figure">
<img src="./images/AIPaper_image4.gif" alt="Figure4" /><p class="caption">Figure4</p>
</div>
<p align="center">


<p><em>Figure  SEQ Figure * ARABIC  OAR Elements in Publishing and Data Dissemination at NIST</em></p>
</p>


<p>The value of efficient internet-based data sharing tools in science and engineering is clear from their widespread adoption by mainstream researchers. In fact,</p>
<p>In addition, no collaborative tools exist today that support a wide collaboration or community interaction on existing data products - what are they?</p>
<p>Where possible this solution will adapt existing IT products, services, and data sharing platforms, but it will be developed specifically to meet the needs and satisfy the constraints and security considerations of research in a federal agency such as NIST.</p>
<h2 id="erb"><a name="ERB"></a>ERB</h2>
<p>In order to publish a paper at NIST, a manuscript must be approved by the Editorial Review System as part of the publishing process. ERB is known as Washtingon ERB (WERB) for NIST Gaithersburg publications, or BERB for NIST Boulder publications. The purpose of editorial review and approval at NIST is to ensure that an effective quality management system is in place and functioning during the process that ends with a technical manuscript. Minimally, most routine errors in typing, spelling, grammar, organization, format, technical expression, and policy should have been detected and corrected in technical manuscripts before submission for Editorial Review Board review. All technical manuscripts are required to submit a Form NIST-114, &quot;Manuscript Review and Approval&quot;. This includes having a several reviewers to insure manuscripts are reviewed for compliance with NIST technical requirements, policy, and editorial quality. When necessary, manuscripts are also reviewed for compliance with legal requirements. Review requirements include:</p>
<ol style="list-style-type: decimal">
<li><p>Technical Review: Critical evaluation of the technical content and methodology, statistical treatment of data, uncertainty analysis, use of appropriate reference data and units, and bibliographic references;</p></li>
<li><p>Policy Review: Examination for consistency with NIST statutory authority and operating policies, appropriateness of selected medium for publication, appropriate recognition of sponsors, institutions and persons, and other relevant matters;</p></li>
<li><p>Editorial Review: Examination for correct use of language, freedom from jargon, clarity of expression, effective organization, good format, appropriate title, adequate references, indexing, citations, footnotes, and figures; writing quality, spelling, conformance with Appendix D (required use of SI units), typography, and correct data on Form NIST-114, Manuscript Review and Approval; and</p></li>
<li><p>Legal Review: Examination by the NIST Counsel when required as determined by the supervisor, the OU Director, or the Editorial Review Board.</p></li>
</ol>
<p>There is a Web based document management system called the NIST Integrated Knowledge EditorialNet (NIKE) that is tasked with managing the publication approval process.</p>
<h2 id="computer-software"><a name="Computer_Software"></a>Computer Software</h2>
<p>Computer software too has a WERB vetting process to insure quality. From the NIST Administrative Manual Section 4.09 Appendix I, regarding &quot;Editorial Review of Computer Software Documentation&quot;, it states that the publication of computer software has become an essential means for disseminating the results of an increasing number of NIST technical programs. The need for editorial review as a vital part of quality management for information on machine-readable media has long been recognized. This need was the focus of a special study by an ad hoc Committee on Software Documentation that reported to WERB on September 30, 1983, and recommended:</p>
<p>&quot;Computer programs can only be sent to NTIS when program documentation exists and when the program documentation has been cleared by WERB.&quot;</p>
<h2 id="midas"><a name="MIDAS"></a>MIDAS</h2>
<p>The Management of Institutional Data Assets System (MIDAS) is a suite of web-based software tools to manage the NIST data publication process containing two components: a Data Management Plan (DMP) tool and an Enterprise Data Inventory (EDI) tool.</p>
<p>NIST's Public Access Policy requires the creation of a DMP for all NIST research data to enhance the integrity and visibility of NIST measurements, standards, and research activities. DMPs enable NIST scientists and researchers to document their plans for storage, archival, and accessibility for NIST's numerous types of information. DMPs are living documents that can be modified as plans and work are updated.</p>
<p>NIST has developed EDI, a catalog system that provides NIST scientists with a means of entering metadata about their datasets, along with an export capability that facilitates the feeding of that metadata into the Federal government's online data catalog, data.gov, as well as NIST's online data catalog, data.nist.gov, where the data can then be discovered by the public.</p>
<div class="figure">
<img src="./images/AIPaper_image5.gif" alt="Figure5" /><p class="caption">Figure5</p>
</div>
<p align="center">


<p><em>Figure  SEQ Figure * ARABIC  MIDAS Open Data Process at NIST – Source</em></p>
</p>


<p> REF <em>Ref14180202  shows the process of a MIDAS application. MIDAS contains a wizard that guides users through creation of a Data Management Plan. MDIAS allows users to upload datasets to a storage location for public access, e.g., by data.gov or data.nist.gov. At this point, MIDAS establishes an automatic digital object identifier (DOI) for use in publishing the dataset, which goes public after final approval. MIDAS includes as a web accessible interface that creates a record for datasets listed in NIST's EDI. A review process is required by MIDAS for publishing data. Finally, MIDAS an export that sends that inventory to  HYPERLINK &quot;http://www.data.gov&quot; &quot;</em>blank&quot;  to allow for public discovery of NIST's data assets.</p>
<p>MNIDAS provides for the automated generation of Digital Object Identifier (DOI) for metadata records so that all records will now be assigned a DataCite DOI, which will increase the visibility of the datasets. A DOI is available to reference as soon as the record is created and will be minted using your homepage URL when the record is approved for publication.</p>
<h2 id="data.gov"><a name="Data.gov"></a>Data.gov</h2>
<p>Since NIST is a Federal government agency under the Department of Commerce, any data can eventually be added to the catalog hosted by Data.gov. The NIST MIDAS services supports this integration into Data.gov.</p>
<p>Data.gov data.gov is managed and hosted by the U.S. General Services Administration, Technology Transformation Service. Under the terms of the 2013 Federal Open Data Policy, newly-generated government data is required to be made available in open, machine-readable formats, while continuing to ensure privacy and security. Data.gov is primarily a federal open government data site but state, local, and tribal governments can also syndicate metadata describing their open data resources on Data.gov.</p>
<p>Data.gov does not host data directly, but rather aggregates metadata about open data resources in one centralized location. Data.gov follows the &quot;Project Open Data&quot; schema -- a set of required fields (Title, Description, Tags, Last Update, Publisher, Contact Name, etc.) for every data set displayed on Data.gov. Once an open data source meets the necessary format and metadata requirements, Data.gov synchronized with that source's metadata on Data.gov.</p>
<p>Preparing a data source for harvesting by the Data.gov catalog differs depending on the type of source:</p>
<p>Federal Data with Project Open Data The most common source is the Public Data Listing as required by the Federal Open Data Policy.</p>
<p>Federal Geospatial Data A number of federal agencies hold geospatial data which has separate requirements under different legal authorities.</p>
<p>Non-Federal Data Non-federal sources are not covered by the Federal Open Data Policy, but can be included in the Data.gov catalog voluntarily. Depending on your platform, creating this harvester might just be the push of a button or it could take a little more work, but the team will walk you through it either way. (See:  HYPERLINK &quot;https://www.data.gov/local/add&quot; &quot;_blank&quot;  )</p>
<p>Once a data source has been established, it is necessary to coordinate with Data.gov using the following steps:</p>
<p><strong>Contact Data.gov.</strong> You first contact Data.gov ( HYPERLINK &quot;mailto:datagov@gsa.gov&quot; ) and include a link to your metadata in the data.json format.</p>
<p><strong>Connect</strong> to the Data source Data.gov will create a new &quot;Harvest Source&quot; that will automatically collect information about your datasets and update Data.gov whenever changes are made on your data catalog.</p>
<p><strong>Testing Data.gov</strong> then tests to ensure the harvester works properly. If anything seems wrong, the Data.gov will help you configure your data catalog so that Data.gov can properly collect your datasets.</p>
<p><strong>Online Worldwide</strong> Once the harvester has been tested successfully, Data.gov will start automatically consuming information about your datasets and all the basic details of your datasets will be available on Data.gov with links to the source and your open data policy.</p>
<h2 id="metadata"><a name="Metadata"></a>Metadata</h2>
<p>Metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information resource <span class="citation">[@press2004understanding]</span>. A dataset is an identifiable collection of structured data objects unified by some criteria (authorship, subject, scope, spatial or temporal extent...). A catalog is a collection of descriptions of datasets; each description is a metadata record. The intention of a data catalog is to facilitate data access by users who are first interested in a particular kind of data, and upon finding a fit-for-purpose dataset, will next want to know how to get the data.</p>
<p>Developing robust metadata has several requirements.</p>
<p>define and name standard metadata fields so that a data consumer has sufficient information to process and understand the described data.</p>
<p>use a standardized regular format for providing detailed information documenting the structure, processing history, quality, relationships, and other properties of a dataset.</p>
<p>if possible, make metadata machine readable which will greatly increase its utility, but requires more detailed standardization</p>
<p>Establishing a common data vocabulary is the key to effective data communication, such that, defining not only field names, but also how information is encoded in the metadata fields is important. The metadata schema specified in this NIST Midas system is based on Data catalog vocabulary (DCAT) <span class="citation">[@maali2014data]</span>, a hierarchical vocabulary specific to datasets. This specification defines three types of metadata elements: Required, Required-if (conditionally required), and Expanded fields. These elements were selected to represent information that is most often looked for on the web. To assist users of other metadata standards, field mappings to equivalent elements in other standards are provided.</p>
<p>Datasets can be stored in various formats, included Commas Separated Value (CSV), proprietary and open-source database schema - either SQL or non-SQL, and various other assorted systems. Regardless of which data format is employed, it should include a fundamental export option which allows the catalog to be exported as a CSV file, which can then be imported into the CSV Converter for conversation to an appropriately formatted JSON file [<a href="#Reference_40">40</a>].</p>
<p>Various tools exist to convert CSV into JSON format in order to satisfy the needed data.json file. Project open data supplies a free CSV Converter to JSON. Comprehensive Knowledge Archive Network (CKAN) is an open-source tool for generating data.json, to help in fostering the sharing of data between public institutions seeking to share their data with the general public. Socrata is a commercial database tool that natively generates data.json in support of data-centered collaboration between governments and the private sector.</p>
<h1 id="disclaimer"><a name="DISCLAIMER"></a>DISCLAIMER</h1>
<p>Commercial equipment and software, many of which are either registered or trademarked, are identified in order to adequately specify certain procedures. In no case does such identification imply recommendation or endorsement by the National Institute of Standards and Technology, nor does it imply that the materials or equipment identified are necessarily the best available for the purpose.</p>
<h1 id="glossary"><a name="Glossary"></a>Glossary</h1>
<p>The following terms are bandied about in Neural nets and Machine Learning. Although the terminology is not difficult, understanding the terminology aids in understanding the technology. The glossaru is a work-in-progress that relies on existing online glossaries, tutorials, publications and Wikipedia [<a href="#Reference_41">41</a>] [<a href="#Reference_42">42</a>] [<a href="#Reference_43">43</a>][<a href="#Reference_44">44</a>] [<a href="#Reference_45">45</a>].</p>
<p><strong>Activation Function</strong> Activation functions are mathematical equations that determine the output of a neural network.The function is attached to each neuron in the network, and determines whether it should be activated (&quot;fired&quot;) or not, based on whether each neuron's input is relevant for the model's prediction. Activation functions also help normalize the output of each neuron to a range between <span class="math">1</span> and <span class="math">0</span> or between <span class="math"> − 1</span> and <span class="math">1</span>.</p>
<p><strong>Backpropagation</strong> is short for &quot;the backward propagation of errors&quot; since an error is computed at the output and distributed backwards throughout the networks layers.</p>
<p><strong>Bias</strong> Allow models to represent data input patterns that do not pass through the origin.</p>
<p><strong>Block</strong> instead of neurons has components that make it smarter than classical neuron and a memory for recent sequences consisting of gates that manage the block's state and output. Block has three types o gates : Forget, input and output. Each unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure. Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.</p>
<p><strong>Classification</strong> Predicting a categorical output, such as yes/no, type, etc. A mapping from unlabeled instances to (discrete) classes.</p>
<p><strong>Data mining</strong> is the process of using historical databases to improve subsequent decision making.</p>
<p><strong>Epoch</strong> An epoch describes the number of times the neural net algorithm sees the entire data set.</p>
<p><strong>Long Short Term Memory</strong> or LTSM is a variant of the RNN architecture. It was difficult to train models using traditional RNN architectures. Recent advancements demonstrate state of the art results using LSTM and BRNN (Bidirectional RNN). The LSTM architecture was able to take care of the vanishing gradient problem in the traditional RNN.</p>
<p><strong>Machine Learning</strong> computer systems that automatically improve through experience <span class="citation">[@Jordan255]</span>.</p>
<p><strong>Recurrent Neural Network</strong> or RNN is a type of neural network designed to handle sequence dependence (e.g., timed ordering of data). The decision a recurrent network reached at time step t-1 affects the decision it will reach one moment later at time step t. RNN is a special case of neural network similar to convolutional neural networks, the difference being that RNN's can retain its state of information.</p>
<p><strong>Softma</strong></p>
<p><strong>Supervised Learning</strong> Training a model using a labeled dataset.</p>
<p><strong>Test Set</strong> set of observations at the end of training to evaluate the efficacy of the learning model.</p>
<p><strong>Training Set</strong> a set of observations used to generate machine learning models.</p>
<p><strong>Unsupervised Learning</strong> Training a model to find patterns in an unlabeled dataset (e.g., clustering).</p>
<p><strong>Unsupervised learning</strong> - technique that groups instances without a pre-specified dependent attribute. Clustering algorithms are usually unsupervised.</p>
<h1 id="references">References</h1>
<p>[1] <a name="Reference_1"></a>M. I. Jordan, and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245): pp. 255-260, 2015. https://science.sciencemag.org/content/349/6245/255.</p>
<p>[2] <a name="Reference_2"></a>L. G. Valiant. A theory of the learnable. In Proceedings of the sixteenth annual ACM symposium on Theory of computing, pp. 436-445, 1984. http://www.montefiore.ulg.ac.be/~geurts/Cours/AML/Readings/Valiant.pdf.</p>
<p>[3] <a name="Reference_3"></a>V. Chandrasekaran, and M. I. Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the National Academy of Sciences, 110(13): pp. E1181--E1190, 2013. https://www.pnas.org/content/pnas/110/13/E1181.full.pdf.</p>
<p>[4] <a name="Reference_4"></a>J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11): pp. 1238-1274, 2013. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.910.7004&amp;rep=rep1&amp;type=pdf.</p>
<p>[5] <a name="Reference_5"></a>NIST High Performance Computing for AI/ML. Artificial Intelligence and Machine Learning HPC at NIST. https://aihpc.ipages.nist.gov/pages/. Accessed: 16-May-2019.</p>
<p>[6] <a name="Reference_6"></a>Kaggle. Datasets. https://www.kaggle.com/datasets. Accessed: 16-May-2019.</p>
<p>[7] <a name="Reference_7"></a>Visual Data. Discover Computer Vision Dataset. https://www.visualdata.io/. Accessed: 16-May-2019.</p>
<p>[8] <a name="Reference_8"></a>. The home of the U.S. Government’s open data. https://www.data.gov/. Accessed: 16-May-2019.</p>
<p>[9] <a name="Reference_9"></a>University of California Irvine. UC Irvine Machine Learning Repository. http://mlr.cs.umass.edu/ml/. Accessed: 16-May-2019.</p>
<p>[10] <a name="Reference_10"></a>P. H. Winston. Artificial intelligence, volume 3. Addison-Wesley Reading, MA, 1992.</p>
<p>[11] <a name="Reference_11"></a>E. W. Staley, K. D. Katyal, and P. Burlina. DRL Based Intelligent Joint Manipulator and Viewing Camera Control for Reaching Tasks and Environments with Obstacles and Occluders. In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-7, 2018. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489273.</p>
<p>[12] <a name="Reference_12"></a>Webster Dictionary. Learning. https://www.merriam-webster.com/dictionary/learning. Accessed: 16-May-2019.</p>
<p>[13] <a name="Reference_13"></a>H. A. Simon. Why should machines learn?. In Machine learning, pp. 25-37, Elsevier, 1983.</p>
<p>[14] <a name="Reference_14"></a>H. A. Pierson, and M. S. Gashler. Deep learning in robotics: a review of recent research. Advanced Robotics, 31(16): pp. 821-835, 2017. https://arxiv.org/pdf/1707.07217.</p>
<p>[15] <a name="Reference_15"></a>W. Böhmer, J. T. Springenberg, J. Boedecker, M. Riedmiller, and K. Obermayer. Autonomous learning of state representations for control: An emerging field aims to autonomously learn state representations for reinforcement learning agents from their real-world sensor observations. KI-Künstliche Intelligenz, 29(4): pp. 353-362, 2015. http://www.redaktion.tu-berlin.de/fileadmin/fg215/articles/boehmer15b.pdf.</p>
<p>[16] <a name="Reference_16"></a>P. Kormushev, S. Calinon, and D. Caldwell. Reinforcement learning in robotics: Applications and real-world challenges. Robotics, 2(3): pp. 122-148, 2013. https://www.mdpi.com/2218-6581/2/3/122/pdf.</p>
<p>[17] <a name="Reference_17"></a>R. Laroche, and R. Féraud. Reinforcement Learning Algorithm Selection. arXiv preprint arXiv:1701.08810, 2017. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.7494&amp;rep=rep1&amp;type=pdf.</p>
<p>[18] <a name="Reference_18"></a>J. Mahler, and K. Goldberg. Learning deep policies for robot bin picking by simulating robust grasping sequences. In Conference on Robot Learning, pp. 515-524, 2017. http://proceedings.mlr.press/v78/mahler17a/mahler17a.pdf.</p>
<p>[19] <a name="Reference_19"></a>A. D'Souza, S. Vijayakumar, and S. Schaal. Learning inverse kinematics. In Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180), 1pp. 298-303 vol.1, Nov 2001. https://s3.amazonaws.com/academia.edu.documents/30756918/10.1.1.60.8175.pdf?response-content-disposition=inline%3B%20filename%3DLearning_inverse_kinematics.pdf&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20190724%2Fus-east-1%2.</p>
<p>[20] <a name="Reference_20"></a>V. Paxson, G. Almes, J. Mahdavi, and M. Mathis. RFC 2330: Framework for IP performance metrics. Jun 1998. https://tools.ietf.org/html/rfc2330.</p>
<p>[21] <a name="Reference_21"></a>W. M. Trochim, and J. P. Donnelly. Types of Reliability from the The Research Methods Knowledge Base. 2006. http://www.anatomyfacts.com/research/researchmethodsknowledgebase.pdf. Accessed: 16-May-2019.</p>
<p>[22] <a name="Reference_22"></a>JCGM. International vocabulary of basic and general terms in metrology (VIM). 2004pp. 9-14, 2004. http://depa.fquim.unam.mx/amyd/archivero/VOCABULARIOINT.DEMETROLOGIA(VIM)_26484.pdf.</p>
<p>[23] <a name="Reference_23"></a>. Evaluation of measurement data - Guide to the expression of uncertainty in measurement (GUM). 50pp. 134, 2008. https://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf.</p>
<p>[24] <a name="Reference_24"></a>P. Lanthier. Understanding the Difference Between Reliability and Availability. https://reliabilityweb.com/tips/article/understanding_the_difference_between_reliability_and_availability/. Accessed: 16-May-2019.</p>
<p>[25] <a name="Reference_25"></a>P. Hamill. Unit test frameworks: tools for high-quality software development. O'Reilly Media, Inc., 2004.</p>
<p>[26] <a name="Reference_26"></a>G. Taguchi. Quality engineering (Taguchi methods) for the development of electronic circuit technology. IEEE Transactions on Reliability, 44(2): pp. 225-229, July 1995.</p>
<p>[27] <a name="Reference_27"></a>B. W. Israelsen, and N. R. Ahmed. “Dave...I Can Assure You ...That It’s Going to Be All Right ...” A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships. ACM Comput. Surv., 51(6): pp. 113:1--113:37, Feb 2019. http://doi.acm.org/10.1145/3267338.</p>
<p>[28] <a name="Reference_28"></a>Data Flair. Train and Test Set in Python Machine Learning – How to Split. https://data-flair.training/blogs/train-test-set-in-python-m. Accessed: 16-May-2019.</p>
<p>[29] <a name="Reference_29"></a>. Cross-validation: evaluating estimator performance. https://scikit-learn.org/stable/modules/cross_validation.html. Accessed: 16-May-2019.</p>
<p>[30] <a name="Reference_30"></a>F. Bonsignorio. A new kind of article for reproducible research in intelligent robotics [from the field]. IEEE Robotics &amp; Automation Magazine, 24(3): pp. 178-182, 2017.</p>
<p>[31] <a name="Reference_31"></a>F. Bonsignorio, A. P. Del Pobil, and E. Messina. Fostering progress in performance evaluation and benchmarking of robotic and automation systems [tc spotlight]. IEEE Robotics &amp; Automation Magazine, 21(1): pp. 22-25, 2014. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6763376.</p>
<p>[32] <a name="Reference_32"></a>E. Guglielmelli. Research Reproducibility and Performance Evaluation for Dependable Robots [From the Editor's Desk]. IEEE Robotics &amp; Automation Magazine, 22(3): pp. 4-4, 2015.</p>
<p>[33] <a name="Reference_33"></a>International Organization Standardization. ISO 26324:2012(en) Information and documentation - Digital object identifier system. 2012.</p>
<p>[34] <a name="Reference_34"></a>ORCID.org. ORCID Connecting Research and Researchers. orcid.org. Accessed: 16-May-2019.</p>
<p>[35] <a name="Reference_35"></a>Y. Joshi. DOI for papers; ORCID for authors. www.editage.com/insights/doi-for-papers-orcid-for-authors. Accessed: 16-May-2019.</p>
<p>[36] <a name="Reference_36"></a>CodeOcean. Discover and run scientific code. codeocean.com. Accessed: 16-May-2019.</p>
<p>[37] <a name="Reference_37"></a>J. L. Ayala. Code Ocean Is Live: Upload Your Algorithms. IEEE Design Test, 34(3): pp. 108-109, July 2017.</p>
<p>[38] <a name="Reference_38"></a>Papers With Code. Automatic SOTA (state-of-the-art) extraction. Accessed: 16-May-2019.</p>
<p>[39] <a name="Reference_39"></a>Australian Research Data Commons (ARDC). Findable, Accessible, Interoperable and Reusable (FAIR) self-assessment tool. https://www.ands-nectar-rds.org.au/fair-tool.</p>
<p>[40] <a name="Reference_40"></a>Project Open Data. CSV to JSON Converter. https://project-open-data.cio.gov/v1.1/metadata-resources/. Accessed: 16-May-2019.</p>
<p>[41] <a name="Reference_41"></a>R. Kohavi, and F. Provost. Glossary of Terms. http://deeplearning.lipingyang.org/wp-content/uploads/2016/12/Glossary-of-Terms-Journal-of-Machine-Learning.pdf. Accessed: 16-May-2019.</p>
<p>[42] <a name="Reference_42"></a>F. Provost, and R. Kohavi. Glossary of terms. Journal of Machine Learning, 30(2-3): pp. 271-274, 1998.</p>
<p>[43] <a name="Reference_43"></a>C. Ma, H. H. Zhang, and X. Wang. Machine learning for big data analytics in plants. Trends in plant science, 19(12): pp. 798-808, 2014.</p>
<p>[44] <a name="Reference_44"></a>L. Carroll. The grossman-cormack glossary of technology-assisted review. Federal Courts Law Review, 7(1): 2013. http://www.wlrk.com/webdocs/wlrknew/AttorneyPubs/WLRK.22191.13.pdf.</p>
<p>[45] <a name="Reference_45"></a>T. Minka. A statistical learning/pattern recognition glossary. Retrieved June, 29pp. 2008, 2005. http://alumni.media.mit.edu/~tpminka/statlearn/glossary/glossary.html.</p>
